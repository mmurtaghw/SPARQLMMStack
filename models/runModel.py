import time
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from accelerate import init_empty_weights, infer_auto_device_map

def load_model(model_name, quantization='none'):
    """
    Load the model and tokenizer with the specified quantization level and device mapping.
    
    Args:
        model_name (str): Name of the model to load.
        quantization (str): Quantization level, options are 'none', '4bit', or '8bit'.
        
    Returns:
        tuple: Loaded model and tokenizer.
    """
    print(model_name)
    with init_empty_weights():
        model = AutoModelForCausalLM.from_pretrained(model_name)
    device_map = infer_auto_device_map(model, max_memory={0: "24GB", 1: "24GB", 2: "24GB", 3: "24GB"})
    print(f"Using device map: {device_map}")

    if quantization == '8bit':
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            load_in_8bit=True
        )
    elif quantization == '4bit':
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            load_in_4bit=True
        )
    else:
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto"
        )

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    return model, tokenizer

def generate_sparql(model, tokenizer, input_text):
    """
    Generate a SPARQL query from a given input prompt using the provided model and tokenizer.
    
    Args:
        model (transformers.PreTrainedModel): Pre-trained model for text generation.
        tokenizer (transformers.PreTrainedTokenizer): Tokenizer corresponding to the model.
        input_text (str): The input prompt to generate a SPARQL query.
    
    Returns:
        tuple: Generated SPARQL query (str) and time taken (float).
    """
    print("Tokenizing input...")
    inputs = tokenizer(input_text, return_tensors="pt").to("cuda")
    start_time = time.time()
    outputs = model.generate(
    **inputs,
    max_new_tokens=150,
    temperature=0.2,  # Reduce randomness
    top_p=0.9,        # Constrain output probabilities
    pad_token_id=tokenizer.eos_token_id
    )
    end_time = time.time()
    print("Decoding output...")
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_text, end_time - start_time


def validate_sparql(model, tokenizer, question, generated_query):
    """
    Use the next LLM in the stack to validate the generated SPARQL query.

    Args:
        model (transformers.PreTrainedModel): Pre-trained model for text generation.
        tokenizer (transformers.PreTrainedTokenizer): Tokenizer corresponding to the model.
        question (str): The original question posed.
        generated_query (str): The SPARQL query generated by the previous LLM.
    
    Returns:
        tuple: Validation result (bool: True if correct, False if incorrect), and time taken (float).
    """
    print("Creating validation prompt...")
    validation_prompt = (
        f"The following SPARQL query was generated for the question:\n\n"
        f"Question: {question}\n"
        f"SPARQL Query: {generated_query}\n\n"
        "Does this SPARQL query correctly answer the question? "
        "Evaluate it based on the query's syntax and relevance to the question.\n\n"
        "Respond with 'Yes' if the query is correct and meets the criteria, or 'No' otherwise. "
    )

    
    # Tokenize input prompt
    print("Tokenizing validation input...")
    inputs = tokenizer(validation_prompt, return_tensors="pt").to("cuda")
    
    # Generate response
    start_time = time.time()
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,
        temperature=0.2,  # Reduce randomness
        top_p=0.9,        # Constrain output probabilities
        pad_token_id=tokenizer.eos_token_id
    )
    end_time = time.time()
    
    # Decode and extract only the response
    print("Decoding validation output...")
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
    validation_response = generated_text[len(validation_prompt):].strip().lower()
    print(validation_response)
    
    # Parse response for "Yes" or "No"
    is_correct = "yes" in validation_response.split("\n")[0]  # Extract first meaningful line
    return is_correct, end_time - start_time

